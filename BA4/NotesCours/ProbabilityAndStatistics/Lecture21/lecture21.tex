% !TeX program = lualatex
% Using VimTeX, you need to reload the plugin (\lx) after having saved the document in order to use LuaLaTeX (thanks to the line above)

\documentclass[a4paper]{article}

% Expanded on 2023-05-11 at 13:20:19.

\usepackage{../../style}

\title{ProbaStats}
\author{Joachim Favre}
\date{Jeudi 11 mai 2023}

\begin{document}
\maketitle

\lecture{21}{2023-05-11}{It's fun we see this theorem that late}{
\begin{itemize}[left=0pt]
    \item Explanation that a random variable converging in distribution to a constant also converges in distribution.
    \item Explanation of Slutsky's lemma. 
    \item Explanation and justification of the weak law of large numbers.
    \item Explanation and justification of the central limit theorem.
\end{itemize}

}

\parag{Example 1}{
    Let $X_1, \ldots, X_n$ be independent variables following the same distribution, with mean $\mu$ and variance $\sigma^2$. Let us consider their mean: 
    \[\frac{1}{n} \sum_{i=1}^{n} X_i = \bar{X}_n\]
    
    We want to show that $\bar{X}_n \over{\to}{2} \mu$. In other words, we want to show that the following converges to $0$: 
    \[\exval\left(\left(\bar{X}_n - \mu\right)^2\right)\]
    
    However, we note that: 
    \[\exval\left(\bar{X}_n\right) = \frac{1}{n}\cdot n \exval\left(X_i\right) = \mu\]
    
    We thus see that: 
    \[\exval\left(\left(\bar{X}_n - \mu\right)^2\right) = \Var\left(\bar{X}_n\right) = \Var\left(\frac{1}{n} \sum_{i=1}^{n} X_i\right) = \frac{1}{n^2} n \Var\left(X_i\right) = \frac{\sigma^2}{n}\]
    
    This indeed goes to 0 as $n \to \infty$.
}

\parag{Example 2}{
    Let $Z \followsdistr \mathcal{N}\left(0, 1\right)$ and $X_n = \left(-1\right)^n Z$. We want to show that $X_n \over{\to}{D} Z$ but that they do not converge in any other mode.

    Let us begin with showing that $X_n \over{\to}{D} Z$. Since the Gaussian is symmetric, $\phi\left(x\right) = \phi\left(-x\right)$, we see that: 
    \[F_{X_n}\left(x\right) = F_{Z}\left(x\right), \mathspace \forall x, n\]
    
    This indeed yields that:
    \[\lim_{n \to \infty} F_{X_n}\left(x\right) = F_Z\left(x\right)\]
    
    Now, let us now show that $X_n \over{\not\to}{P} Z$. We see that, if $n$ is odd: 
    \[\prob\left(\left|X_n - Z\right| > \epsilon\right) = \prob\left(-2Z\right) > \epsilon = \prob\left(\left|Z\right| > \frac{\epsilon}{2}\right) = 2\Phi\left(-\frac{\epsilon}{2}\right)\]

    However, since this does not go to zero as $n \to \infty$, we have indeed shown that $X_n \over{\not\to}{P} Z$.
}

\parag{Recall: Continuity theorem}{
    Let $\left\{X_n\right\}, X$ be random variables with cumulative distribution functions $\left\{F_n\right\}, F$ and which MGFs $M_n\left(t\right), M\left(t\right)$ exist at all $\left|t\right| < b$, for some $b > 0$ (i.e. they exist in a neighbourhood of $t = 0$).

    If there exists a $a \in \mathbb{R}$ such that $0 < a < b$ and $\lim_{n \to \infty} M_n\left(t\right) = M\left(t\right)$ for all $\left|t\right| \leq a$, then $X_n \over{\to}{D} X$.
    
    \subparag{Intuition}{
        In other words, if $\lim_{n \to \infty} M_n\left(t\right) = M\left(t\right)$ in a neighbourhood of $t = 0$, then $F_n\left(x\right) \to F\left(x\right)$ at each $x \in \mathbb{R}$ where $F$ is continuous. 
    }
}

\parag{Example}{
    Let $X$ be a random variable with a geometric distribution with a probability of success $p$. We want to calculate the limit distribution of $pX$ as $p \to 0$.

    We have always seen limits of sequences, now we are considering a continuous limit. A way to tackle this is to define $p = \frac{1}{n}$, giving us $X_n = \frac{X}{n}$. This supposes that the limit converges and is thus not perfect, but it works. However, let us do this proof completely formally here, without making any assumption on the convergence.

    Let us consider the MGF: 
    \autoeq{\exval\left(e^{tpX}\right) = \sum_{x=1}^{\infty} e^{tpx}p\left(1-p\right)^{x-1} = pe^{t p} \sum_{x=0}^{\infty} \left(e^{t p} \left(1 - p\right)\right)^x = \frac{p e^{t p}}{1 - \left(1 - p\right) e^{t p}} = \frac{p}{e^{-t p} - 1 + p} = \frac{1}{1 + \frac{e^{- t p} - 1}{p}} \to \frac{1}{1 - t}}
    for $\left|t\right| < 1$, using the particular limit $\lim_{x \to \infty} \frac{e^{x} - 1}{x} = 1$.
    
    We notice that this is the MGF of $Y \followsdistr \exp\left(1\right)$. By the continuity theorem, we showed that $\frac{1}{p}X \over{\to}{D} Y$.
}

\parag{Theorem}{
    Let $x_0$ be a constant, $X, Y, \left\{X_n\right\}, \left\{Y_n\right\}$ be random variables and $h$ a function continuous at $x_0$.

    We have the following implication:
    \[X_n \over{\to}{D} x_0 \implies h\left(X_n\right) \over{\to}{P} h\left(x_0\right)\]
    
    \subparag{Implication}{
        In particular, we have:
        \[X_n \over{\to}{D} x_0 \implies X_n \over{\to}{P} x_0\]
    }
    
    \subparag{Remark}{
        We know that we always have: 
        \[X_n \over{\to}{P} X \implies X_n \over{\to}{D} X\]
        
        The converse is wrong in general. However, when we have $X = x_0$ constant, then this theorem tells us that we have both directions.
    }

    \subparag{Proof}{
        We want to show the particular case where: 
        \[X_n \over{\to}{D} c \implies X_n\over{\to}{P} c\]
        
        Since $X_n \over{\to}{D} c$, we know by definition that: 
        \[\lim_{n \to \infty} F_n\left(x\right) = F\left(x\right)\]
        where the CDF of a constant is a step function:
        \begin{functionbypart}{F\left(x\right) = \prob\left(X \leq c\right)}
            1, & x > c \\
            0, & x < c 
        \end{functionbypart}

        In particular, this means that, for any $\epsilon > 0$: 
        \autoeq{\lim_{n \to \infty} \prob\left(\left|X_n - c\right| < \epsilon\right) = \lim_{n \to \infty} \prob\left(c - \epsilon < X_n < c + \epsilon\right) = \lim_{n \to \infty} \left(F_n\left(c + \epsilon\right) - F_n\left(c - \epsilon\right)\right) = F\left(c + \epsilon\right) - F\left(c - \epsilon\right) = 1 - 0 = 1}

        This indeed yields that $X_n \over{\to}{P} c$.

        \qed
    }
    
}

\parag{Slutsky's lemma}{
    Let $x_0, y_0$ be constants, and $X, Y, \left\{X_n\right\}, \left\{Y_n\right\}$ be random variables.

    If $X_n \over{\to}{D} X$ and $Y_n \over{\to}{P} y_0$, then we have: 
    \[X_n + Y_n \over{\to}{D} X + y_0\]
    
    Similarly, if $X_n \over{\to}{D} X$ and $Y_n \over{\to}{P} y_0$, then: 
    \[X_n Y_n \over{\to}{D} X y_0\]
}

\parag{Example}{
    Let $X_1, \ldots, X_n$ be IID with mean $\mu_X$ and variance $\sigma_X^2$, and let $Y_1, \ldots, Y_n$ be IID with mean $\mu_Y$ and variance $\sigma_Y^2$. We also suppose that $\mu_Y \neq 0$.

    We want to show that:
    \[\frac{\bar{X}_n}{\bar{Y}_n} \over{\to}{P} \frac{\mu_X}{\mu_Y}\]

    We have already shown that $\bar{X}_n \over{\to}{P} \mu_X$ and $\bar{Y} \over{\to}{P} \mu_Y$. In particular, we have $\bar{Y}^{-1}_n \over{\to}{D} \mu_Y^{-1}$ which gives us that: 
    \[\frac{\bar{X}_n}{\bar{Y}_n} \over{\to}{D} \frac{\mu_X}{\mu_Y}\]
    
    However, by our theorem, this tells us that:
    \[\frac{\bar{X}_n}{\bar{Y}_n} \over{\to}{P} \frac{\mu_X}{\mu_Y}\]
}

\parag{Degenerate convergence}{
    Let $X_1, \ldots, X_n \iid F$ and $M_n = \max\left\{X_1, \ldots, X_n\right\}$.

    We already know that: 
    \[\prob\left(M_n \leq x\right) = F\left(x\right)^n\]
    
    However, as $n \to \infty$, this tends towards:
    \begin{functionbypart}{\prob\left(M_n \leq x\right)}
        0, & F\left(x\right) < 1 \\
        1, & F\left(x\right) = 1
    \end{functionbypart}
    since $b^n \to 0$ for any $0 \leq b < 1$.

    This is a degenerate limit distribution, so we may want to center and scale: 
    \[Y_n = \frac{M_n - b_n}{a_n}\]
    
    There sometimes exists sequences $a_n, b_n$ such that the distribution is not degenerate.
}

\parag{Example}{
    Let $X_1, \ldots, X_n \iid \exp\left(\lambda\right)$, and let $M_n$ be their maximum. We want to find $a_n, b_n$ such that: 
    \[Y_n = \frac{M_n - b_n}{a_n} \over{\to}{D} Y\]
    where $Y$ has a non-degenerate distribution.

    We notice that: 
    \[\prob\left(Y_n \leq y\right) = \prob\left(M_n \leq a_n y + b_n\right) = F\left(b_n + a_n y\right)^n = \left(1 - \exp\left(-b_n \lambda - a_n \lambda y\right) \right)^n\]
    
    To simplify everything, we decide to pick $a_n = \frac{1}{\lambda}$ and $b_n = \frac{\log\left(n\right)}{\lambda}$, giving: 
    \[\prob\left(Y_n \leq y\right) = \left(1 - \frac{\exp\left(-y\right)}{n}\right)^n \to \exp\left(-\exp\left(y\right)\right)\]
    
    This is indeed a non-degenerate distribution (which is named the Gumbel distribution).
}

\subsection{Adding many random variables}

\parag{Theorem: Weak law of large numbers}{
    Let $X_1, X_2, \ldots$ be IID random variables with finite expectation $\mu$. Also, let us consider their average: 
    \[\bar{X}_n = \frac{X_1 + \ldots + X_n}{n}\]
    
    Then, we have: 
    \[\bar{X}_n \over{\to}{P} \mu\]
    
    \subparag{Remark}{
        We have already shown that, if the $X_1, \ldots$ have a finite variance: 
        \[\bar{X}_n \over{\to}{2} \mu\]
        
        However, this theorem does not require this hypothesis.
    }

    \subparag{Remark 2}{
        This theorem can be generalised to the strong law of large numbers which says that, under the same hypothesis:
        \[\bar{X}_n \over{\to}{a.s.} \mu\]

        This is not important to know.
    }

    \subparag{Justification}{
        We want to make a proof where we don't take the assumption that $\sigma^2 < \infty$. We will use MGFs to do so; we assume convergence, even though we would need to prove it for a formal proof. Let us consider the MGF of $\bar{X}_n$: 
        \autoeq{M_{\bar{X}_n}\left(t\right) = \exval\left(\exp\left(\frac{t}{n} \sum_{i=1}^{n} X_i\right)\right) = M_{X_1}\left(\frac{t}{n}\right)^n = \left(1 + \frac{t}{n} \exval\left(X_1\right) + \frac{t^2 \exval\left(X_1^2\right)}{2n^2} + \ldots\right)^n}
        since the random variables are independent.

        Now, we notice that: 
        \[M_{\bar{X}_n}\left(t\right) = \left(1 + \frac{t}{n} \exval\left(X_1\right) + \frac{t^2 \exval\left(X_1^2\right)}{2n^2} + \ldots\right)^n \to e^{t \exval\left(X_1\right)}\]
        since $\left(1  + \frac{\lambda}{n} + \frac{c}{n^2}\right)^n \to e^{\lambda}$, everything which decays faster than $\frac{1}{n}$ is just not considered (this is something else we would need to show for a complete proof).
        
        However, we recognise the MGF of the degenerate random variable $Z = \exval\left(X_1\right)$ constant: 
        \[M_Z\left(t\right) = \exval\left(e^{tZ}\right) = \exval\left(e^{t \exval\left(X_1\right)}\right) = e^{t\exval\left(X_1\right)}\]
        
        We have thus shown by the continuity theorem that: 
        \[\frac{1}{n} \sum_{i=1}^{n} X_i = \bar{X}_n \over{\to}{D} \exval\left(X_1\right)\]
        
        However, since $\exval\left(X_1\right)$ is constant, we can use our theorem to get: 
        \[\frac{1}{n} \sum_{i=1}^{n} X_i \over{\to}{P} \exval\left(X_1\right)\]
    }
}

\parag{Observation}{
    Let us consider IID random variables $X_1, \ldots$ such that $\exval\left(X_i\right) = 0$ and $\Var\left(X_i\right) = 1$.

    We have seen with the law of large numbers that: 
    \[\frac{1}{n}\sum_{i=1}^{n} X_i \over{\to}{P} \exval\left(\frac{1}{n}\sum_{i=1}^{n} X_i\right) = 0\]
    
    Indeed, the variance shrinks to 0. Let us consider another scaling, so that it does not do so: 
    \[Y_n = \frac{1}{\sqrt{n}} \sum_{i=1}^{n} X_i\]
    
    This implies that:  
    \[\exval\left(Y_n\right) = \exval\left(\frac{1}{\sqrt{n}}\sum_{i=1}^{n} X_i\right) \frac{1}{\sqrt{n}}\cdot n \exval\left(X_i\right) = 0\]
    
    \[\Var\left(Y_n\right) = \Var\left(\frac{1}{\sqrt{n}} \sum_{i=1}^{n} X_i\right) = \frac{1}{n} \cdot n\Var\left(X_i\right) = 1\]

    This leads to the following theorem.
}

\parag{Lemma}{
    Let $X_1, X_2, \ldots$ be independent (but not necessarily identically distributed) random variables which all have the expectation 0 and variance 1. Also, let $Z \followsdistr \mathcal{N}\left(0, 1\right)$.

    Then, we have that: 
    \[Z_n = \frac{1}{\sqrt{n}} \sum_{i=1}^{n} X_n = \sqrt{n}\cdot \bar{X} \over{\to}{D} Z\]
    
    \subparag{Justification}{
        Let us consider the MGF of $Z_n = \frac{1}{\sqrt{n}}\sum_{i=1}^{n} X_i$: 
        \autoeq{M_{Z_n}\left(t\right) = \exval\left(\exp\left(\frac{t}{\sqrt{n}} \sum_{i=1}^{n} X_i\right)\right) = \left(M_{X_1}\left(\frac{t}{\sqrt{n}}\right)\right)^n = \left(1 + \frac{t}{\sqrt{n}} \exval\left(X_1\right) + \frac{t^2 \exval\left(X_1^2\right)}{\sqrt{n}^2 \cdot 2} + \ldots\right)^n}

        However, we know by hypothesis that $\exval\left(X_1\right) = \mu = 0$, and:
        \[\exval\left(X_1^2\right) = \exval\left(X_1^2\right) - \underbrace{\exval\left(X_1\right)^2}_{= 0} = \Var\left(X_1\right) = 1\]

        This yields that:
        \[M_{\frac{Y_n}{\sqrt{n}}}\left(t\right) = \left(1 + \frac{t^2}{2n} + \ldots\right)^n \to e^{\frac{t^2}{2}}\]
        by a similar reasoning to the justification of the weak law of large numbers.
        
        However, we recognise the MGF of $\mathcal{N}\left(0, 1\right)$, which indeed implies that $Z_n \over{\to}{D} Z$ by the continuity theorem. Note that we are again hiding the fact that we can really ignore everything $o\left(\frac{1}{n^2}\right)$.
    }
}


\parag{Central limit theorem}{
    Let $X_1, X_2, \ldots$ be independent (but not necessarily identically distributed) random variables which all have the same expectation $\mu$ and non-zero finite variance $\sigma^2$. Also, let $Z \followsdistr \mathcal{N}\left(0, 1\right)$.

    Then, we have that: 
    \[Z_n = \frac{1}{\sqrt{n}} \sum_{i=1}^{n} \frac{X_n - \mu}{\sigma} = \frac{\sqrt{n} \left(\bar{X}_n - \mu\right)}{\sigma} \over{\to}{D} Z\]
    
    \subparag{Implication}{
        This for instance implies that 
        \[\prob\left(\frac{\sqrt{n}\left(\bar{X} - \mu\right)}{\sigma} \leq z\right) \to \prob\left(Z \leq z\right) = \Phi\left(z\right)\]
    }

    \subparag{Proof}{
        Let us consider the variance and mean of $\frac{X_i - \mu}{\sigma}$: 
        \[\exval\left(\frac{X_i - \mu}{\sigma}\right) = \frac{\exval\left(X_i\right) - \mu}{\sigma} = \frac{\mu - \mu}{\sigma} = 0\]
        \[\Var\left(\frac{X_i - \mu}{\sigma}\right) = \frac{1}{\sigma^2} \Var\left(X_i - \mu\right) = \frac{1}{\sigma^2} \Var\left(X_i\right) = \frac{\sigma^2}{\sigma^2} = 1\]

        This means that we can just use our lemma on those shifted random variables.

        \qed
    }
}

\end{document}
