\documentclass[a4paper]{article}

% Expanded on 2021-10-11 at 13:11:48.

\usepackage{../../style}

\title{Algèbre linéaire}
\author{Joachim Favre}
\date{Lundi 11 octobre 2021}

\begin{document}
\maketitle

\lecture{6}{2021-10-11}{Fin des applications, et produit matriciel}{
\begin{itemize}[left=0pt]
    \item Définition de surjectivité et d'injectivité d'une application linéaire.
    \item Résumé des théorèmes liés aux pivots des matrices, ainsi que la surjectivité ou injectivité des transformation linéaires correspondantes.
    \item Définition des opérations matricielles, et explication de la logique derrière le produit matriciel.
\end{itemize}

}

\parag{Exemple d'autres transformations du plan}{
    On a vu que les rotations sont une transformation linéaire. Cependant, il y en a d'autres comme des dilatations horizontales et verticales, les réflexions selon une droite quelconque (évêque ? (ouai je la refais)), les projections, etc.
}


\subsection{Injectivité et surjectivité d'applications}

\parag{Définition de surjectivité}{
    On dit qu'une application $T: \mathbb{R}^{n} \mapsto \mathbb{R}^{m}$ est \important{surjective} si tout vecteur de $\mathbb{R}^{m}$ est l'image \emph{d'au moins un} vecteur de $\mathbb{R}^{n}$.
}

\parag{Définition d'injectivité}{
    On dit qu'une application $T: \mathbb{R}^{n} \mapsto \mathbb{R}^{m}$ est \important{injective} si tout vecteur de $\mathbb{R}^{m}$ est l'image \emph{d'au plus un} vecteur de $\mathbb{R}^{n}$. En d'autres mots, $T\left(\bvec{x}\right) = \bvec{b}$ a au plus une solution pour tout $\bvec{b}$.
}

\parag{Théorème}{
    Soit $T: \mathbb{R}^{n} \mapsto \mathbb{R}^{m}$ une application linéaire. $T$ est injective si et seulement si l'équation
    \[T\left(\bvec{x}\right) = \bvec{0}\]
    admet la solution triviale, $\bvec{x} = \bvec{0}$, pour unique solution.

    \subparag{Preuve}{
        \begin{enumerate}[left=0pt]
            \item Supposons que $T$ est injective.

                On sait que de $T\left(\bvec{0}\right) = \bvec{0}$ puisque $T$ est linéaire. De plus, comme $T$ est injective, l'équation $T\left(\bvec{x}\right) = \bvec{0}$ a au plus une solution. De ce fait, elle ne peut pas avoir d'autre solution que l'équation triviale.

            \vspace{1em}

            \item Supposons que $T\left(\bvec{x}\right) = \bvec{0}$ a pour seule solution $\bvec{x} = \bvec{0}$.

                Puisque que $T$ est une application linéaire, on sait qu'il existe une matrice $A$ telle que $T\left(\bvec{x}\right) = A \bvec{x}$.

               De plus, on sait que $T$ est injective si et seulement si $A \bvec{x} = \bvec{b}$ a au plus une solution pour tout $\bvec{b}$. Comme $T\left(\bvec{x}\right) = \bvec{0}$ a une unique solution, $A \bvec{x} = \bvec{0}$ a une unique solution.

               On sait que toutes les solutions de $A \bvec{x} = \bvec{b}$ s'écrivent sous la forme
               \[\bvec{x} = \bvec{p} + \bvec{h}\]
               avec $\bvec{p}$, une solution de $A \bvec{x} = \bvec{b}$ (si elle existe) et $\bvec{h}$, toutes les solutions de $A \bvec{x} = \bvec{0}$. Or, $\bvec{h} = \bvec{0}$ uniquement, donc on en déduit que $T\left(\bvec{x}\right) = A \bvec{x} = \bvec{b}$ a au plus une solution.

               Ainsi, cela implique que $T$ est injective.
        \end{enumerate}

        \qed
    }
}

\parag{Théorème}{
    Soit $T: \mathbb{R}^{n} \mapsto \mathbb{R}^{m}$ une application linéaire, et $A$ la matrice canoniquement associée à $T$ ($T\left(\bvec{x}\right) = A \bvec{x}$). Alors:
    \begin{enumerate}
        \item $T$ est surjective si et seulement si les colonnes de $A$ engendrent $\mathbb{R}^{m}$.
        \item $T$ est injective si et seulement si les colonnes de $A$ sont linéairement indépendantes.
    \end{enumerate}

    Pour nos preuves, prenons
    \[T\left(\bvec{x}\right) = A \bvec{x} = x_1 \bvec{a}_1 + \ldots + x_n \bvec{a}_n\]
    où $\bvec{a}_1, \ldots, \bvec{a}_n$ sont les colonnes de la matrice $A$.

    \subparag{Preuve de la partie (1)}{
        $T$ est surjective si et seulement si pour tout $\bvec{b}$ dans $\mathbb{R}^{m}$, il existe $\bvec{x}$ tel que $T\left(\bvec{x}\right) = \bvec{b}$. Les colonnes de $A$ engendrent $\mathbb{R}^{m}$ si et seulement si pour $\bvec{b}$ dans $\mathbb{R}^{m}$ il existe $x_1, \ldots, x_n$ tel que $x_1 \bvec{a}_1 + \ldots + x_n \bvec{a}_n = \bvec{b}$. Les deux sont donc bel et bien équivalentes.

        Note personnelle: on a déjà démontré un théorème qui dit que $A \bvec{x} = \bvec{b}$ a au moins une solution si et seulement si les colonnes de $A$ engendrent $\mathbb{R}^{m}$, voir le théorème avec quatre propositions équivalentes.
    }

    \subparag{Preuve de la partie (2)}{
        Par le théorème précédent, on sait que $T$ est injective si et seulement si $T\left(\bvec{x}\right) = \bvec{0}$ admet la solution triviale comme unique solution. Donc, $T$ est injective si et seulement si
        \[x_1 \bvec{a}_1 + \ldots + x_n \bvec{a}_n = \bvec{0}\]
        admet $\bvec{x} = \bvec{0}$ comme unique solution (ce qui est la définition d'indépendance linéaire).

        \qed
    }
}

\parag{Résumé personnel}{
    Soient $A \in \mathbb{R}^{m \times n}$, une matrice, et $T$ l'application linéaire définie telle que
    \[T\left(\bvec{x}\right) = A \bvec{x}\]

    Les propriétés suivantes sont équivalentes:
    \begin{enumerate}
        \item $T$ est surjective.
        \item Il existe dans chaque ligne de $A$ une position pivot.
        \item Pour tout $\bvec{b} \in \mathbb{R}^{m}$, l'équation $A \bvec{x} = \bvec{b}$ a au moins une solution.
        \item Tout vecteur de $\mathbb{R}^{m}$ est une combinaison linéaire des colonnes de $A$.
        \item Les colonnes de $A$ engendrent $\mathbb{R}^{m}$.
    \end{enumerate}

    \vspace{1em}
    De la même manière, les propriétés suivantes sont aussi équivalentes:
    \begin{enumerate}
        \item $T$ est injective.
        \item Il existe dans chaque colonne de $A$ une position pivot.
        \item Pour tout $\bvec{b} \in \mathbb{R}^{m}$, l'équation $A \bvec{x} = \bvec{b}$ a au plus une solution.
        \item Les colonnes de $A$ sont linéairement indépendantes.
        \item $T$ admet la solution triviale pour unique solution.
        \item Aucune des colonnes de $A$ n'est une combinaison linéaire de celles qui la précèdent.
    \end{enumerate}

    \vspace{1em}
    Nous avons aussi les conditions suivantes qui nous permettent de savoir instantanément si une famille de $p$ vecteurs de dimension $n$ sont linéairement dépendants:
    \begin{enumerate}
        \item Un des vecteur est le vecteur nul.
        \item $p > n$.
    \end{enumerate}

}

\section{Matrices}
\subsection{Opérations matricielles}
\parag{Coefficients d'une matrice}{
    Pour une matrice $A \in \mathbb{R}^{m\times n}$ ($m$ lignes et $n$ colonne), on note $a_{ij} \in \mathbb{R}$ le coefficient $\left(i, j\right)$ de $A$, c'est-à-dire le coefficient dans la $i$-ème ligne et $j$-ème colonne.

    On note souvent les colonnes de $A$ par $\bvec{a}_1, \ldots, \bvec{a}_n \in \mathbb{R}^{m}$ de la manière suivante:
    \[A = \begin{bmatrix}  &  &  \\ \bvec{a}_1 & \ldots & \bvec{a}_n \\  &  &  \end{bmatrix} \]

    En particulier, $a_{ij}$ est le $i$-ème coefficient du vecteur colonne $\bvec{a}_j$. Les coefficients diagonaux $a_{11}$, $a_{22}$, \ldots forment la \important{diagonale principale de $A \in \mathbb{R}^{m \times n}$}
}


\parag{Matrices remarquables}{
    Une \important{matrice diagonale} est une matrice carrée de taille $n \times n$ dont les entrées non diagonales sont nulles. En d'autres mots, $i \neq j \implies a_{ij} = 0$. Par exemple, la matrice identité est diagonale:
    \[I_n = \begin{bmatrix} 1 &  &  &  \\  & 1 &  &  \\  &  & \ddots &  \\  &  &  & 1 \end{bmatrix} \]
    (quand un coefficient est nul, on ne le note pas forcément).

    Une autre matrice utile est la matrice suivante:
    \[0 = \begin{bmatrix} 0 & \ldots & 0 \\ \vdots &  & \vdots \\ 0 & \ldots & 0 \end{bmatrix} \in \mathbb{R}^{m \times n}\]

    Sa taille est définie selon le contexte
}

\parag{Somme de deux matrices}{
    Étant données deux matrices $A, B \in \mathbb{R}^{m \times n}$ de même taille (sinon la somme n'est pas définie). On définit leur somme comme la matrice de taille $m \times n$ notée $A + B$ telle que l'entrée $\left(i, j\right)$ de $A + B$ (qu'on peut aussi écrire $\left(A + B\right)_{ij}$) est égale à $a_{ij} + b_{ij}$.

    \subparag{Exemple}{
        Si on a
        \[A = \begin{bmatrix} -1 & 0 & 0 \\ 2 & 1 & 3 \end{bmatrix}, \mathspace B = \begin{bmatrix} 3 & 1 & 7 \\ 1 & -2 & 0 \end{bmatrix} \]

        Alors, leur somme est donnée par :
        \[A + B = \begin{bmatrix} -1 & 0 & 0 \\ 2 & 1 & 3 \end{bmatrix} + \begin{bmatrix} 3 & 1 & 7 \\ 1 & -2 & 0 \end{bmatrix} = \begin{bmatrix} 2 & 1 & 7 \\ 3 & -1 & 3 \end{bmatrix} \]
    }
}

\parag{Multiplication d'une matrice par un scalaire}{
    Pour une matrice $A \in \mathbb{R}^{m \times n}$ et un scalaire $r \in \mathbb{R}$, on définit leur \important{produit} comme la matrice de taille $m \times n$ notée $rA$ (il est habituel de mettre le scalaire à gauche de la matrice, mais ce n'est pas obligatoire) telle que l'entrée $\left(i, j\right)$ de $rA$ (aussi notée $\left(rA\right)_{ij}$) est égale à $ra_{ij}$.

    On écrit aussi $-A$ pour $\left(-1\right)A$ et $A - B$ pour $A + \left(-1\right)B$.

    \subparag{Exemple}{
        Si on a
        \[A = \begin{bmatrix} 3 & 2 & 0 \\ 1 & 2 & 4 \end{bmatrix} \text{ et } r = 2\]

        Alors:
        \[rA = 2\begin{bmatrix} 3 & 2 & 0 \\ 1 & 2 & 4 \end{bmatrix} = \begin{bmatrix} 6 & 4 & 0 \\ 2 & 4 & 8 \end{bmatrix} \]
    }
}

\parag{Propriétés}{
    Soit $A, B, C$ des matrices de \important{même taille}, et $r, s$ des scalaires. On a les propriétés suivantes:
    \begin{enumerate}
        \item $A + B = B + A$
        \item $\left(A + B\right) + C = A + \left(B + C\right)$
        \item $A + 0 = A$
        \item $r\left(A + B\right) = rA + rB$
        \item $\left(r + s\right)A = rA + sA$
        \item $r\left(sA\right) = \left(rs\right)A$
    \end{enumerate}
}

\parag{Produit de deux matrices}{
    Le produit intuitif, celui où on deux matrices de tailles $m \times n$, et dont dont l'entrée $\left(i, j\right)$ est égale à $a_{ij} b_{ij}$, n'est pas celui qu'on va prendre. En effet, il s'appelle produit de Hadamard mais est très rarement utile. Le produit qu'on va définir ci-dessous peut sembler plus compliqué, mais il est beaucoup plus utile.

    Dans le langage des transformations linéaires, une matrice est un objet qui transforme un vecteur en un autre. Donc si on transforme $\bvec{x}$ par $B$, on obtient $B \bvec{x}$. Si ensuite on utilise la matrice $A$ pour transformer ce vecteur, on va obtenir $A\left(B \bvec{x}\right)$. On peut donc se demander s'il y aurait pas une matrice qui transforme directement $\bvec{x}$ vers $A\left(B \bvec{x}\right)$. On va appeler cette matrice $AB$.

    En d'autres mots, étant données deux matrices $A, B$ de tailles telles que $A\left(B \bvec{x}\right)$ est bien défini, on cherche une matrice qu'on notera $AB$ telle que $AB \bvec{x} = A\left(B \bvec{x}\right)$ pour tout $\bvec{x}$. Essayons avec $\bvec{x} \in \mathbb{R}^{p}$, $B \in \mathbb{R}^{n \times p}$ et $A \in \mathbb{R}^{m \times n}$. On a:
    \[B \bvec{x} = x_1 \bvec{b}_1 + \ldots + x_p \bvec{b}_p \in \mathbb{R}^{n}\]

    On a pris $A \in \mathbb{R}^{m \times n}$ de manière à ce qu'on puisse encore multiplier le vecteur $B \bvec{x}$ par $A$. On a donc:
    \[A\left(B \bvec{x}\right) = A\left(x_1 \bvec{b}_1 + \ldots + x_p \bvec{b}_p\right) = x_1 A \bvec{b}_1 + \ldots + x_p A \bvec{b}_p\]
    par les propriétés vues ci-dessus. Donc, on en déduit $A\left(B \bvec{x}\right)$ est une combinaison linéaire des vecteurs $A \bvec{b}_1$, \ldots, $A \bvec{b}_p$ avec $x_1, \ldots, x_p$. En notation matricielle, cela s'écrit:
    \[A\left(B \bvec{x}\right) = \underbrace{\begin{bmatrix}  &  &  \\ A \bvec{b}_1 & \ldots & A \bvec{b}_p \\  &  &  \end{bmatrix}}_{\text{On l'appelle } AB} \begin{bmatrix} x_1 \\ \vdots \\ x_p \end{bmatrix} = AB \bvec{x}\]

    Puisque $A$ est $m \times n$ et $B$ est $n \times p$, on a que $AB$ est $m \times p$.

    Note personnelle: en écrivant les dimensions de nos matrices, il est possible de voir facilement la logique de quelle coordonnée de la dimension doit être égale, et de quelle va être la dimension du résultat:
    \begin{center}
    \begin{tabular}{ccccc}
        $A$ & $\cdot$ & $B$ & $=$ & $AB$ \\
        $m \times \color{red}{n}$ & & ${\color{red}n} \times p$ & & $m \times p$
    \end{tabular}
    \end{center}
}

\parag{Définition du produit matriciel}{
    Soit $A$ une matrice $m \times n$ et $B$ une matrice $n \times p$ (les $n$ doivent être les mêmes) dont on note les colonnes $\bvec{b}_1, \ldots, \bvec{b}_p$. On appelle produit de $A$ et $B$, et l'on note $AB$ la matrice $m \times p$ dont les colonnes sont $A \bvec{b}_1, \ldots, A \bvec{b}_p$, c'est-à-dire:
    \[AB = A \begin{bmatrix}  &  &  \\ \bvec{b}_1 & \ldots & \bvec{b}_p \\  &  &  \end{bmatrix} = \begin{bmatrix}  &  &  \\ A \bvec{b}_1 & \ldots & A \bvec{b}_p \\  &  &  \end{bmatrix} \]

    Cette définition est construite de sorte que le produit matriciel correspond à la composition d'applications linéaires.
}

\parag{Exemple}{
    Soient $A$, une matrice $2 \times 3$, $B$ une matrice $3 \times 2$ et $C$ une matrice $2 \times 2$. On a que $A A$ n'est pas définie, $A B$ est une matrice $2 \times 2$, $B A$ une matrice $3 \times 3$, etc.

    Par exemple:
    \[A B = \begin{bmatrix} -1 & 0 & 3 \\ 2 & 1 & 4 \end{bmatrix} \begin{bmatrix} 1 & -1 \\ 2 & 0 \\ 0 & 3 \end{bmatrix}  \]

    Donc,
    \[A \bvec{b}_1 = \begin{bmatrix} -1 \\ 4 \end{bmatrix} \text{ et } A \bvec{b}_2 = \begin{bmatrix} 10 \\ 10 \end{bmatrix} \]

    Ce qui nous donne:
    \[AB = \begin{bmatrix} -1 & 10 \\ 4 & 10 \end{bmatrix} \]
}

\parag{Définition équivalente}{
    Si l'on note $\left(AB\right)_{ij}$ le coefficient $\left(i, j\right)$ de $AB$ et si $A$ est une matrice $m \times n$ alors:
    \[\left(AB\right)_{ij} = a_{i1} b_{1j} + \ldots + a_{1n} b_{nj}\]
}

\parag{Exemple}{
    Si on a
    \[A = \begin{bmatrix} -1 & 0 & 3 \\ 2 & 1 & 4 \end{bmatrix} \text{ et } B = \begin{bmatrix} 1 & -1 \\ 2 & 0 \\ 0 & 3 \end{bmatrix} \]

    Alors:
    \[\left(AB\right)_{11} = a_{11} b_{11} + a_{12} b_{21} + a_{13} b_{31} = -1\cdot 1 + 0\cdot 2 + 3\cdot 0 = -1\]




}

\parag{Note personnelle}{
     Mettre les matrices dans la position suivante, sous forme de grille, m'aide à calculer un produit matriciel (et à connaître la dimension de la matrice d'arrivée).

    \begin{center}
    \begin{tabular}{ccc||cc}
        & & & \textcolor{red}{1} & \textcolor{red}{-1} \\
        & & & \textcolor{red}{2} &  \textcolor{red}{0} \\
        & & & \textcolor{red}{0} &  \textcolor{red}{3} \\
        \hline
        \hline
        \textcolor{blue}{-1} & \textcolor{blue}{0} & \textcolor{blue}{3} & \textcolor{green}{-1} & \textcolor{green}{10} \\
        \textcolor{blue}{2} & \textcolor{blue}{1} & \textcolor{blue}{4} & \textcolor{green}{4} & \textcolor{green}{10}
    \end{tabular}
    \end{center}

    Qui représente:
    \[{\color{blue}A} \cdot {\color{red}B} = {\color{blue}\begin{bmatrix} -1 & 0 & 3 \\ 2 & 1 & 4 \end{bmatrix} } {\color{red}\begin{bmatrix} 1 & -1 \\ 2 & 0 \\ 0 & 3 \end{bmatrix}} = {\color{green}\begin{bmatrix} -1 & 10 \\ 4 & 10 \end{bmatrix}} = {\color{green}AB}\]

    De cette manière, les coefficients de $AB$ sont trouvés en regardant les coefficients de $A$ qui sont sur la même ligne et ceux de $B$ qui sont sur la même colonne. On a par exemple bien que:
    \[\left(AB\right)_{12} = -1\cdot \left(-1\right) + 0\cdot 0 + 3\cdot 3 = 1 + 9 = 10\]
}


\parag{Propriétés}{
    Soit $A$ une matrice $m \times n$ et $B$, $C$ deux matrices telles que la sommes et les produits ci-dessous aient un sens. On a:
   \begin{enumerate}
       \item $A\left(BC\right) = \left(AB\right)C$
       \item $A\left(B + C\right) = AB + AC$
       \item $\left(B + C\right)A = BA + CA$
       \item $r\left(AB\right) = r\left(A\right)B = A\left(rB\right)$ pour tout scalaire $r$
       \item $I_m A = A = A I_n$, d'où le nom ``matrice identité''
   \end{enumerate}

   \subparag{Preuve}{
       Laissée en exercice au lecteur.
   }
}

\parag{Non-propriétés}{
    Attention, il y a certaines propriétés qui tiennent pour les scalaires mais pas pour les matrices. Ainsi:
    \begin{enumerate}
        \item En général, $AB \neq BA$
        \item On ne peut pas simplifier un produit de matrices. Autrement dit, si $AB = AC$, alors il est en général faux que $B = C$.
        \item Si un produit $AB$ est égal à la matrice nulle, on ne peut pas en général en déduire que $A = 0$ ou $B = 0$.
    \end{enumerate}

    \subparag{Exemple}{
        On a par exemple
        \[A = \begin{bmatrix} 1 & -1 \\ 1 & -1 \end{bmatrix} \neq 0, \mathspace B = \begin{bmatrix} 1 & 1 \\ 1 & 1 \end{bmatrix} \neq 0, \mathspace C = \begin{bmatrix} 2 & 2 \\ 2 & 2 \end{bmatrix}\neq 0\]

        Alors:
        \[AB = \begin{bmatrix} 0 & 0 \\ 0 & 0 \end{bmatrix} = AC\]

        Ce qui est un contre-exemple aux propriétés 2 et 3.
    }

}

\parag{Puissance de matrice}{
    Si $A$ est carré, alors les produits $AA$, $A A A$, \ldots\ ont du sens (ce n'est pas le cas si $A$ n'est pas carrée). Donc, on définit la notation :
    \[A^{k} = \underbrace{A A \ldots A A}_{k\text{ fois}} \mathspace k \in \mathbb{N}_0\]

    On appelle $A^{k}$, ``$A$ à la puissance $k$''.  Par convention, on prend
    \[A^{0} = I_n\]

    On a donc $A^{k} \bvec{x}$ = ``multiplier $\bvec{x}$ $k$ fois par $A$'' = ``$A \ldots A \bvec{x}$''.
}

\end{document}
