\documentclass[a4paper]{article}

% Expanded on 2021-10-28 at 08:16:09.

\usepackage{../../style}

\title{Algèbre linéaire}
\author{Joachim Favre}
\date{Jeudi 28 octobre 2021}

\begin{document}
\maketitle

\lecture{11}{2021-10-28}{Retour à l'App. (store) linéaire}{
    \begin{itemize}[left=0pt]
        \item Définition de kernel et d'image pour des applications linéaires.
        \item Définition des applications linéaires d'un espaces vectoriels.
        \item Extension du concept de familles libres et de bases aux applications linéaire d'espaces vectoriels.
        \item Découverte qu'une matrice est inversible si et seulement si ses colonnes forment une base de $\mathbb{R}^{n}$.
    \end{itemize}
}

\parag{Théorème}{
    Si $\bvec{v}_1$, \ldots, $\bvec{v}_p$ sont des vecteurs d'un espace vectoriel $V$, alors $\vect\left\{\bvec{v}_1, \ldots, \bvec{v}_p\right\}$ est un sous-espace vectoriel de $V$.
}

\parag{Exemples}{
    Dans $V = \mathbb{R}^3$, les ensembles suivants sont des sous-espaces:
    \begin{itemize}
        \item L'origine : $\left\{\bvec{0}\right\} = \vect\o$ ;
        \item Une droite passant par l'origine : $\vect{\bvec{v}}$ avec $\bvec{v} \neq \bvec{0}$ ;
        \item Un plan passant par l'origine : $\vect\left\{\bvec{v}_1, \bvec{v}_2\right\}$ avec $\bvec{v}_1$ $\bvec{v}_1, \bvec{v}_2$ linéairement indépendants.
        \item $\mathbb{R}^3$ tout entier, obtenu comme $\vect\left\{\bvec{v}_1, \bvec{v}_2, \bvec{v}_3\right\}$ avec $\bvec{v}_1, \bvec{v}_2, \bvec{v}_3$ linéairement indépendants.
    \end{itemize}

}

\parag{Théorème}{
    L'ensemble des solutions $H$ d'un système homogène $A \bvec{x} = \bvec{0}$ avec $A \in \mathbb{R}^{m \times n}$ est un sous-espace vectoriel de $\mathbb{R}^{n}$.

    \subparag{Preuve}{
        \begin{enumerate}[left=0pt]
            \setcounter{enumi}{-1}
            \item L'ensemble solution est un sous-ensemble de $\mathbb{R}^{n}$ (il faut clairement que $\bvec{x}$ soit en dimension $n$).
            \item $\bvec{0}$ appartient à $H$ puisque $A \bvec{0} = \bvec{0}$
            \item Soient $\bvec{u}, \bvec{v}$ deux solutions quelconques (:p) du système homogène. On a bien:
            \[A\left(\bvec{u} + \bvec{v}\right) = A \bvec{u} + A \bvec{v} = \bvec{0} + \bvec{0} = \bvec{0}\]

            Donc $\bvec{u} + \bvec{v} \in H$

            \item Soit $\bvec{u}$ une solution de $A \bvec{x} = \bvec{0}$, et $c$ un réel quelconque. Alors:
            \[A\left(\bvec{cu}\right) = cA \bvec{u} = c \bvec{0} = \bvec{0}\]

            Donc $c \bvec{u} \in H$.
        \end{enumerate}

    }

}

\subsection{Noyaux, images, et applications linéaires}
\parag{Définition de kernel et d'image}{
    Soit $A \in \mathbb{R}^{m \times n}$ une matrice. On lui associe deux espaces.

    \important{Le noyau de $A$}, ``kernel'' en anglais, l'ensemble solution du système homogène:
    \[\ker A = \left\{\bvec{x} \in \mathbb{R}^{n} \telque A \bvec{x} = \bvec{0}\right\}\]

    C'est un sous-espace de $\mathbb{R}^{n}$.

    \important{L'image de $A$}, le sous-espace engendré par les colonnes de $A$ :
    \[\im A = \vect\left\{\bvec{a}_1, \ldots, \bvec{a}_n\right\} = \left\{A \bvec{x} \in \mathbb{R}^{m} \telque \bvec{x} \in \mathbb{R}^{n}\right\}\]

    En d'autres mots:
    \[\im A = \left\{\bvec{b} \in \mathbb{R}^{m} \telque \exists \bvec{x} \in \mathbb{R}^{n}, A \bvec{x} = \bvec{b}\right\}\]


    C'est un sous-espace de $\mathbb{R}^{m}$.
}

\parag{Exemple}{
    Pour une matrice $A \in \mathbb{R}^{m \times n}$, on a $\im A = \vect\left\{\bvec{a}_1, \ldots, \bvec{a}_n\right\}$. Il est aussi possible d'écrire $\ker A$ comme $\vect\left\{\ldots\right\}$.

    Pour cela, il nous faut trouver une forme paramétrique pour les solutions de $A \bvec{x} = \bvec{0}$.

    On a le système suivant, et sa matrice échelonnée réduite équivalente:
    \[A = \begin{bmatrix} -3 & 6 & -1 & 1 & -7 \\ 1 & -2 & 2 & 3 & -1 \\ 2 & -4 & 5 & 8 & -4 \end{bmatrix} \to \begin{bmatrix} 1 & -2 & 0 & -1 & 3 \\ 0 & 0 & 1 & 2 & -2 \\ 0 & 0 & 0 & 0 & 0 \end{bmatrix}  \]

    Le système suivant est équivalent:
    \begin{systemofequations}{}
    &\ x_1 - 2x_2 - x_4 + 3x_5 = 0  \\
    &\ x_3 + 2x_4 - 2x_5 = 0 \\
    &\ 0 = 0
    \end{systemofequations}

    Or, on a donc:
    \begin{systemofequations}{}
    &\ x_1 = 2x_2 + x_4 - 3x_5 \\
    &\ x_2 = x_2 \\
    &\ x_3 = -2x_4 + 2x_5 \\
    &\ x_4 = x_4 \\
    &\ x_5 = x_5
    \end{systemofequations}

    Qu'on peut aussi noter sous forme paramétrique:
    \[\bvec{x} = \begin{bmatrix} x_1 \\ x_2 \\ x_3 \\ x_4 \\ x_5 \end{bmatrix} = \begin{bmatrix} 2x_2 + x_4 - 3x_5 \\ x_2 \\ -2x_4 + 2x_5 \\ x_4 \\ x_5 \end{bmatrix} = x_2 \begin{bmatrix} 2 \\ 1 \\ 0 \\ 0 \\ 0 \end{bmatrix} + x_4 \begin{bmatrix} 1 \\ 0 \\ -2 \\ 1 \\ 0 \end{bmatrix} + x_5 \begin{bmatrix} -3 \\ 0 \\ 2 \\ 0 \\ 1 \end{bmatrix} \]

    En d'autres mots,
    \[\ker A = \left\{\bvec{x} \telque A \bvec{x} = \bvec{0}\right\} = \vect\left\{\begin{bmatrix} 2 \\ 1 \\ 0 \\ 0 \\ 0 \end{bmatrix}, \begin{bmatrix} 1 \\ 0 \\ -2 \\ 1 \\ 0 \end{bmatrix}, \begin{bmatrix} -3 \\ 0 \\ 2 \\ 0 \\ 1 \end{bmatrix} \right\}\]

    On remarques que le nombre de vecteurs dont on a besoin pour décrire le kernel, 3 ici, est le nombre de variables libres. Ces trois vecteurs sont clairement linéairement indépendants (et le seront toujours ; cela vient du fait qu'il y a que des zéros sur la ligne des variables libres, sauf dans un des vecteurs (par exemple, la deuxième composante des trois vecteurs est nulle, sauf celle du premier)).

}

\subsection{Extension aux applications linéaires}
\parag{Définition application linéaire }{
    On appelle une \important{application linéaire} $T$ d'un espace vectoriel $V$ dans un espace vectoriel $W$ un procédé qui, à tout vecteur de $\bvec{x}$ de $V$, associe un unique vecteur $T\left(\bvec{x}\right)$ de $W$ de façon que:
    \begin{enumerate}
        \item $T\left(\bvec{u} + \bvec{v}\right) = T\left(\bvec{u}\right) + T\left(\bvec{v}\right)$ pour tout $\bvec{u}, \bvec{v} \in V$
        \item $T\left(c \bvec{u}\right) = cT\left(\bvec{u}\right)$ pour tout $\bvec{u}$ de $V$ et tout scalaire $c$.
    \end{enumerate}

}


\parag{Extension}{
    On définit le noyau de $T$ :
    \[\ker T = \left\{\bvec{x} \in V \telque T\left(\bvec{x}\right) = \bvec{0}\right\}\]

    On définit aussi l'image de $T$ :
    \[\im T = \left\{T\left(\bvec{x}\right) \in W \telque \bvec{x} \in V\right\}\]

    On peut montrer que $\ker T$ est un sous-espace de $V$ et $\im T$ est un sous-espace de $W$.

    \subparag{Note}{
        Dans ce cas, on n'a pas $T\left(\bvec{x}\right) = A \bvec{x}$, puisque $V$ et $W$ peuvent ne pas être $\mathbb{R}^{n}$ et $\mathbb{R}^{m}$.
    }

}

\parag{Exemple}{
    Soit $V \in \mathbb{P}_3$ et $W = \mathbb{P}_2$. Soit $T : V \mapsto W$ l'application qui, à un polynôme $p$, fait correspondre sa dérivée $T\left(p\right) = p'$.

    Si $p\left(t\right) = a_0 + a_1 t + a_2 t^2 + a_3 t^3$, alors:
    \[p'\left(t\right) = a_1 + 2a_2 t + 3a_3 t^2\]

    On a bien $p \in \mathbb{P}_3 \implies T\left(p\right) \in \mathbb{P}_2$. De plus, on peut remarquer que c'est bien une application linéaire:
    \begin{enumerate}
        \item Soit $p, q \in \mathbb{P}_3$. On a:
        \[\left(T\left(p\right) + T\left(q\right)\right)\left(t\right) = a_1 + 2a_2 t + 3a_3 t^2 + b_1 + 2b_2 t + 3b_3^2\]

        Or, c'est bien égal à :
        \[\left(a_1 + b_1\right) + 2\left(a_2 + b_2\right)t + 3\left(a_3 + b_3\right)t^2 = T\left(p + q\right)\left(t\right)\]
        \item On peut aussi montrer que $T\left(cp\right) = cT\left(p\right)$ pour tout $p \in \mathbb{P}_3$.

    \end{enumerate}

    On peut maintenant regarder quel est le noyau de $T$. On a :
    \[\ker T = \left\{p \in \mathbb{P}_3 \telque T\left(p\right) = 0\right\} = \mathbb{P}_0 = \left\{\text{polynômes constants}\right\}\]

    De plus, l'image de $T$ est donnée par:
    \[\im T = \left\{q \in \mathbb{P}_2 \telque \exists p \in \mathbb{P}_3, T\left(p\right) = q\right\} = \mathbb{P}_2\]

}

\subsection{Familles libres et bases}
\parag{Définition}{
    Soit $V$ un espace vectoriel. On dit que les vecteurs $\bvec{v}_1, \ldots, \bvec{v}_p$ dans $V$ sont \important{linéairement indépendants} si l'équation
    \[c_1 \bvec{v}_1 + \ldots c_p \bvec{v}_p = \bvec{0}\]
    admet comme seule solution la solution triviale $c_1 = \ldots = c_p = 0$.

    Dans ce cas, on dit que la \important{famille} $\left(\bvec{v}_1, \ldots, \bvec{v}_p\right)$ est \important{libre}. Sinon, on dit que la famille est \important{liée.}
}

\parag{Théorème}{
    Une famille $\left(\bvec{v}_1, \ldots, \bvec{v}_p\right)$ d'au moins deux vecteurs, avec $\bvec{v}_1 \neq \bvec{0}$, est liée si et seulement s'il existe $j > 1$ tel que $\bvec{v}_j$ soit une combinaison linéaire des vecteurs qui viennent avant lui, soit $\bvec{v}_1, \ldots, \bvec{v}_{j-1}$.

    \subparag{Remarque}{
        On l'avait démontré pour $\mathbb{R}^{n}$, mais il fonctionne donc de la même manière pour n'importe quel espace vectoriel. La preuve est similaire à ce qu'on avait fait.
    }

}

\parag{Exemple}{
    Avec $V = \mathbb{P}_2$, on considère $p_1, p_2, p_3 \in \mathbb{P}_2$ définis par :
    \[p_1\left(t\right) = 1, \mathspace p_2\left(t\right) = t, \mathspace p_3\left(t\right) = 4 - t\]

    On remarque qu'ils sont linéairement dépendants puisque
\[\left(p_3 + p_2 - 4p_1\right)\left(t\right) = p_3\left(t\right) + p_2\left(t\right) - 4p_1\left(t\right) = 4 - t + t - 4 = 0 \]
    pour tout $t$. (Le ``pour tout $t$'' est important, sinon on a juste une équation en $t$).
}

\parag{Exemple 2}{
    Avec $V = \mathbb{P}_2$, on considère $p_1, p_2, p_3 \in \mathbb{P}_2$ définis par :
    \[p_1\left(t\right) = 1, \mathspace p_2\left(t\right) = t, \mathspace p_3\left(t\right) = t^2\]

    On veut résoudre:
    \[c_1 p_1 + c_2 p_2 + c_3p_3 = 0\]

    On a:
    \[\left(c_1 p_1 + c_2 p_2 + c_3p_3\right)\left(t\right) = c_1 p_1\left(t\right) + c_2 p_2\left(t\right) + c_3 p_3\left(t\right) = c_1 + c_2 t + c_3 t^2\]

    On sait que \textit{\important{le seul polynôme de $\mathbb{P}_n$ qui a strictement plus que $n$ racines est le polynôme nul}}. Ainsi, nécessairement, chaque coefficient doit être nul. On a donc:
    \[c_1 = c_2 = c_3 = 0\]

    Donc, les polynômes $p_1, p_2, p_3$ sont linéairement indépendants.

}

\parag{Définition des bases}{
    Soit $H$ un sous-espace de $V$ (on pourrait très bien avoir $H = V$).


    Une famille de vecteurs $\left(\bvec{b}_1, \ldots, \bvec{b}_p\right)$ de $V$ est une \important{base de $H$} si:
    \begin{enumerate}
        \item La famille est libre : les vecteurs $\bvec{b}_1, \ldots, \bvec{b}_p$ sont linéairement indépendants
        \item La famille engendre $H$ : $H = \vect\left(\bvec{b}_1, \ldots, \bvec{b}_p\right)$, mais ni plus ni moins.
    \end{enumerate}

   \subparag{Remarque}{
       En d'autres mots, une base est le nombre minimum de vecteurs qui engendrent l'espace vectoriel. Si on en rajoute un, alors ils ne seront plus linéairement indépendants.
   }
}

\parag{Observations}{
    \begin{enumerate}[left=0pt]
        \item Si $\bvec{b}_1, \ldots, \bvec{b}_p \in V$ sont linéairement indépendants, alors ils forment une base pour $\vect\left(\bvec{b}_1, \ldots, \bvec{b}_p\right)$.
        \item Si $\bvec{b}_1, \ldots, \bvec{b}_p \in V$ forment une base pour un sous-espace $H$, alors les vecteurs $\bvec{b}_1, \ldots, \bvec{b}_p$ sont dans $H$ car $H = \vect\left(\bvec{b}_1, \ldots, \bvec{b}_p\right)$.
    \end{enumerate}

}

\parag{Exemples et non-exemples}{
    \begin{enumerate}[left=0pt]
        \item Si on a :
        \[V = \mathbb{R}^2, \mathspace \bvec{b}_1 = \begin{bmatrix} 1 \\ 0 \end{bmatrix}, \mathspace \bvec{b}_2 = \begin{bmatrix} 0 \\ 1 \end{bmatrix} \]

        Ils sont linéairement indépendants, et forment une base pour $\vect\left(\bvec{b}_1, \bvec{b}_n\right) = \mathbb{R}^2$.
        \item Si on a :
        \[V = \mathbb{R}^3, \mathspace \bvec{b}_1 = \begin{bmatrix} 1 \\ 1 \\ 0 \end{bmatrix}, \mathspace \bvec{b}_2 = \begin{bmatrix} 1 \\ -1 \\ 0 \end{bmatrix} \]

        Ils sont bien linéairement indépendants, donc ils forment une base pour l'espace qu'ils engendrent, qui est $\vect\left\{\bvec{b}_1, \bvec{b}_2\right\}$, le plan $O_{xy}$ dans $\mathbb{R}^3$.
        \item Si on a :
        \[V = \mathbb{R}^2, \mathspace \bvec{b}_1 = \begin{bmatrix} 1 \\ 0 \end{bmatrix}, \mathspace \bvec{b}_2 = \begin{bmatrix} 1 \\ 1 \end{bmatrix}, \mathspace \bvec{b}_3 = \begin{bmatrix} 0 \\ -1 \end{bmatrix} \]
        Ils sont linéairement dépendants puisque $3 > 2$. Donc, ils ne forment pas une base.
        \item Si on a :
        \[V = \mathbb{P}_2, \mathspace \bvec{b}_1\left(t\right) = 1, \mathspace \bvec{b}_2\left(t\right) = t, \mathspace \bvec{b}_3\left(t\right) = t^2\]

        Ils sont linéairement indépendants, donc ils forment une base pour $\vect\left\{\bvec{b}_1, \bvec{b}_2, \bvec{b}_3\right\} = \mathbb{P}_2$.
        \item Si on a:
        \[V = \mathbb{P}_3, \mathspace \bvec{b}_1\left(t\right) = 1, \mathspace \bvec{b}_2\left(t\right) = t, \mathspace \bvec{b}_3\left(t\right) = t^2\]

        Ils sont linéairement indépendants, donc ils forment une base pour $\vect\left\{\bvec{b}_1, \bvec{b}_2, \bvec{b}_3\right\} = \mathbb{P}_2 \subset \mathbb{P}_3$.


    \end{enumerate}

}

\parag{Matrices inversibles}{
    On remarque que si on a $A \in \mathbb{R}^{n \times n}$ inversible, alors les colonnes de $A$ forment une base de $\mathbb{R}^{n}$, puisque ses colonnes sont donc linéairement indépendantes et engendrent $\mathbb{R}^{n}.$

    Dans l'autre sens, si la matrice n'est pas inversible, alors les colonnes de $A$ ne sont pas linéairement indépendantes, donc elles forment pas de base.

    En d'autres mots, une matrice est inversible si et seulement si ses colonnes forment une base de $\mathbb{R}^{n}$.
}

\parag{Base canonique de $\mathbb{R}^{n}$}{
    Les colonnes de la matrice identité $I_n$ forment une base de $\mathbb{R}^n$, puisqu'elle est inversible, on l'appelle la \important{base canonique de $\mathbb{R}^{n}$}.

    Par exemple, dans $\mathbb{R}^{3}$:
    \[\bvec{e}_1 = \begin{bmatrix} 1 \\ 0 \\ 0 \end{bmatrix}, \mathspace \bvec{e}_2 = \begin{bmatrix} 0 \\ 1 \\ 0 \end{bmatrix}, \mathspace \bvec{e}_3 = \begin{bmatrix} 0 \\ 0 \\ 1 \end{bmatrix} \]

}





\end{document}
