\documentclass[a4paper]{article}

% Expanded on 2021-10-27 at 15:21:26.

\usepackage{../../style}

\title{AICC 1}
\author{Joachim Favre}
\date{Mercredi 27 octobre 2021}

\begin{document}
\maketitle

\lecture{12}{2021-10-27}{Greed, coins and same-sex marriages}{
    \begin{itemize}[left=0pt]
        \item Explanation of the concept of greedy algorithms.
        \item Explanation of the Cashier's algorithm, and proof of its optimality for American and European denominations.
        \item Definition of matching, stable matching and unstable matching.
        \item Explanation of the marriage problem, and explanation of the Gale-Shapley algorithm, which allows to always find a stable matching (in the context of the marriage problem).
    \end{itemize}

}

\subsection{Optimisation algorithms}
\parag{Goal}{
    The goal is to minimise or maximise some parameter over all possible inputs. For example, if we want to find a route between two cities, determine how to encode messages with the fewest bits, and so on.
}

\parag{Greedy algorithms}{
    They are very short-sighted, and the only take the ``best'' choice at every step.

    This does not always gives us the optimal result. Sometimes, we have to go down a hill to get up a mountain. However, in many instances it does. Proving that the greedy algorithm is correct is required to use it.
}

\parag{Example}{
    \imagehere{GreedyAlgorithmTreesExample.png}

    Firstly, the task is to find the longest path to get to the bottom of the left tree. A greedy algorithm would always go to the node with the greatest node, and it would work perfectly.

    Now, let's say that the task to find the leaf with the largest value in the right tree (for example if it is a minimax algorithm for chess, we give a value to each position, and we want to get to the best in the long run). We will not go to the right leaf. Maybe sacrificing a knight allows us to get a rook.
}

\begin{filecontents*}[overwrite]{cashieralgorithm.code}
procedure change(c : array of n > 0 values of coins, where c[1] > c[2] > ... > c[n])
    for i := 1 to r
        d[i] := 0  // number of coins of value c[i]
        while n >= c[i]
            d[i] := d[i] + 1
            n := n - c[i]  // modify amount left to give
    return d
\end{filecontents*}

\parag{Cashier's algorithm}{
    Let's say we have coins of different values. Coins can be for example ``quarters'' (25 cents), ``dimes'' (10 cents), ``nickels'' (5 cents) and ``pennies'' (1 cent). We want to be able to give a certain value using the least possible number of coins.

    At each step, we choose the coin with the highest possible value that does not exceed the amount left.

    \importcode{cashieralgorithm.code}{pseudo}
}

\parag{Example}{
    Let's say we want to change for $n = 67$ cents. The highest coin available is $25$, so we have 42 left. We can give a second quarter, and we have $17$ cents left to give. Then we can give a dime, a nickel, a penny and another penny.
}

\parag{Proof of optimality}{
    We want to show that the change making algorithm for US coins is optimal.

}

\parag{Lemma}{
     The change of $n > 0$ cents using the least possible number of coins amongst quarters, dimes, nickels, and pennies
    \begin{enumerate}
        \item has at most 2 dimes, 1 nickel and 4 pennies;
        \item cannot have 2 dimes and 1 nickel;
        \item the total amount of change in dimes, nickels, and pennies cannot exceed 24 cents.
    \end{enumerate}

    \subparag{Proof of part (1)}{
        Let's suppose for contradiction that we have 3 dimes = 30 cents. Then we can replace them by $25 + 5$ cents, which represent two coins.

        2 nickels = 10 cents can be replaced by one coin of 10 cents, and 5 pennies = 5 cents can be replaced by one coin of 5.
    }

    \subparag{Proof of part (2)}{
        We cannot have 2 dimes and 1 nickel, since we could replace them by one coin of 25.
    }

    \subparag{Proof of part (3)}{
        Dimes nickels and pennies in total have less than 25, since else we could replace them by using a coin of 25.

        Another way of seeing this is to see that we have at most 2 dimes, 1 nickel and 4 pennies, but we cannot have 2 dimes and 1 nickel, so we thus need to get rid of the penny. In other words, we have 2 dimes and 4 nickels, which represent 24 cents.
    }



}

\parag{Theorem (optimality)}{
    The greedy change-making algorithm for US coins produces change using the fewest coins possible.

    \subparag{Proof}{
        Let $c_{25}, c_{10}, c_5, c_1$ the number of coins of the ``optimal'' solution and $\bar{c}_{25}, \bar{c}_{10}, \bar{c}_5, \bar{c}_1$ be the number of coins the algorithm gives.

        Since the ``optimal'' solution is optimal, we know $\bar{c}_{25} \geq c_{25}$. Let's assume for contradiction that our program did not produce an optimal solution, meaning $\bar{c}_{25} > c_{25}$. It would then mean that the optimal solution would reach the same value, $n$, by using one less 25 cents coin. Thus, it needs to compensate this difference with the other coins:
        \[10 c_{10} + 5c_5 + c_1 > 24\]

        This is a contradiction by the above lemma.

        The same way, assuming that $\bar{c}_{10} > c_{10}$, then we would have
        \[5c_5 + c_1 > 9\]
        which is also a contradiction.

        Again, assuming that $\bar{c}_5 \geq c_5$, then we would have $c_1 > 5$ which is another contradiction. We thus also conclude that $\bar{c}_1 = c_1$.
    }

}

\parag{Available coins}{
    Optimality depends on the denominations available. If we have ``quarters'' (25 cents), ``dimes'' (10 cents) and ``pennies'' (1 cent), then the algorithm would not produce the minimum number of coins. For example, if we want to reach 31, it will use one quarter and 6 pennies, whereas we can reach it in 3 dimes and 1 penny.
}

\parag{Europe denominations}{
    In Europe we have $1, 5, 10, 20, 50$.

    \subparag{Lemma}{
        The solution with the smallest coins
        \begin{enumerate}
            \item has at most $4\times1$, $1\times5$, $1\times10$, $2\times20$;
            \item cannot have $2\times20$ and $1\times10$ at the same time;
            \item the total amount using $1, 5, 10, 20, 50$ cannot exceed 49.
        \end{enumerate}
    }

    We can then use this lemma to prove this denomination's optimality, using a proof similar to the one hereinabove.
}


\parag{Dropping coins}{
    For denominations, $c_1, \ldots c_n$ where $c_i | c_{i+1}$. The greedy algorithm is optimal (it is a sufficient condition, but not necessary).

    \subparag{Example for Europe}{
        We see that it is not a necessary condition, since $20$ does not divide $50$. However, if we drop the $20$, then we know that it definitely works for $1, 5, 10, 50$. However, for $1, 20, 50$ the greedy algorithm would not work (for example, when doing the change for 69 (nice) coins).
    }
}

\subsection{Stable matching}
\parag{Goal}{
    The task is to pair elements from equally sized two groups considering their preferences for members of the other group so that there are no ways to improve the preferences.
}

\parag{Definition of matching}{
    Given a finite set $A$, a \important{matching} of $A$ is a set of (unordered) pairs of distinct elements of $A$ where any elements occurs in at most one pair (such pairs are called independent)

    A \important{maximum matching} is a matching that contains the largest possible number of independent pairs.
}

\parag{Example}{
    Let $A = \left\{1, 2, 3, 4\right\}$
   \begin{itemize}
       \item $\left\{1,2\right\}$ and $\left\{\left(1,3\right),\left(2,4\right)\right\}$ are matchings.
       \item $\left\{2,2\right\}$ and $\left\{\left(1,2\right),\left(2,4\right)\right\}$ are not matchings.
       \item $\left\{\left(1,3\right),\left(2,4\right)\right\}$ is a maximum matching.
   \end{itemize}

   Let $B = \left\{1, 2, 3, 4, 5\right\}$
   \begin{itemize}
       \item $\left\{\left(1,3\right), \left(2,4\right)\right\}$ and $\left\{\left(5,3\right),\left(2,4\right)\right\}$ are two different maximum matchings.
   \end{itemize}

}

\parag{Definition of preferences}{
    A \important{preference list} $L_x$ defines for every element $x \in A$ the order in which the element prefers to be paired with. $x \in A$ prefers $y$ to $z$ if $y$ precedes $z$ on $L_x$.

    \subparag{Example}{
        For example, if we have $A = \left\{\text{Lou, Glenn, Bobbie, Tyler}\right\}$. Then, we could have $L_{\text{Tyler}} = \left\{\text{Bobbie, Lou, Glenn}\right\}$.
    }

}

\parag{Stability of matching}{
    A matching is \important{unstable} if there are two pairs $\left(x, y\right),\left(v, w\right)$ in the matching such that $x$ prefers $v$ to $y$ and $v$ prefers $x$ to $w$. In other words, each want to leave their partner.

    A \important{stable} matching is a matching that is not unstable.

    \subparag{Example}{
        For example, if we have $A = \left\{\text{Lou, Glenn, Bobbie, Tyler}\right\}$, and:
        \begin{itemize}
            \item $L_{\text{Lou}} = \left\{\text{Glenn, Bobbie, Tyler}\right\}$
            \item $L_{\text{Glenn}} = \left\{\text{Bobbie, Lou, Tyler}\right\}$
            \item $L_{\text{Bobbie}} = \left\{\text{Lou, Glenn, Tyler}\right\}$
            \item $L_{\text{Tyler}} = \left\{\text{Lou, Glenn, Bobbie}\right\}$
        \end{itemize}

        Let's begin with $\left\{\left(\text{\textcolor{red}{Glenn}, Lou}\right), \left(\text{\textcolor{red}{Bobbie}, Tyler}\right)\right\}$. It is unstable since Glenn prefers Bobbie over Lou, and Bobbie prefers Glenn over Tyler.

        Reordering our matching, we get $\left\{\left(\text{Glenn, \textcolor{red}{Bobbie}}\right), \left(\text{\textcolor{red}{Lou}, Tyler}\right)\right\}$. It is unstable as well since Bobbie prefers to be with Lou, and Lou prefers to be with Bobbie.

        Then, our new matching is $\left\{\left(\text{\textcolor{red}{Lou}, Bobbie}\right), \left(\text{\textcolor{red}{Glenn}, Tyler}\right)\right\}$. Again, Lou prefers to be with Glenn, and Glenn prefers to be with Lou.

        If we reorder another time our matchings, we will have come back to the initial matching, which shows that this will go on forever. It is possible that there does not exist any stable matching. (No one wants to stay with Tyler, poor them.)
    }
}

\parag{Marriage problem}{
    To guarantee the existence of a stable maximum matching, irrespective of the preference lists, it suffices to use a more stringent (harsher) pairing rule.

    Given a set with even cardinality, we partition $A$ into two disjoint subsets $A_1$ and $A_2$, where $A_1 \cup A_2 = A$ and $\left|A_1\right| = \left|A_2\right|$. A matching is a bijection from the elements of one set to the elements of the other set.

    Put in simpler words, pairs can only consist of one element of $A_1$ and one of $A_2$ each. There is no way for an element from $A_1$ to be matched with another element of $A_1$.

    The professor gave the example of boys and girls getting married, but let's use families instead. If we have two families $A_1$ and $A_2$, then members from one family cannot get married with a member of they own family (unless they come from northern France).
}

\begin{filecontents*}[overwrite]{stablematching.code}
let M be the set of pairs under construction
Initially M = empty set
while |M| < |A1|:
    select an unpaired element x in A1
    let x propose to the first element y of A2 on Lx
    if y is unpaired then add the pair (x, y) to M
    else  // y is paired already
        let x' in A1 be the element that y is paired to
        if x' precedes x on Ly then remove y from Lx
        else  // x precedes x' on Ly
            replace (x', y) by (x, y) from M, and remove y from Lx'
\end{filecontents*}

\parag{Existence of stable maximum matching}{
    A greedy algorithm to construct a stable maximum matching for the marriage problem can be used to prove existence of stable matching. It is very efficient, and it gave a Nobel price to its creator. It is named the Gale-Shapley algorithm.

    \importcode{stablematching.code}{pseudo}
}

\parag{Example}{
    Let's say we have $A_1 = \left\{x_1, x_2, x_3, x_4\right\}$ and $A_2 = \left\{y_1, y_2, y_3, y_4\right\}$. Moreover, let
    \[L_{x_i} = \left\{y_1, y_2, y_3, y_4\right\}\]
   \[L_{y_1} = \left\{x_2, x_1, x_3, x_4\right\}\]
   \[L_{y_2} = \left\{x_4, x_3, x_2, x_1\right\}\]
   \[L_{y_3} = \left\{x_2, x_3, x_4, x_1\right\}\]
   \[L_{y_4} = \left\{x_4, x_1, x_2, x_3\right\}\]

   Let's pick $x_1$. Since it prefers $y_1$ over everyone else, and since $y_1$ is not paired yet, then we add $\left(x_1, y_1\right)$ to M.

   Now, picking $x_2$, it also prefers $y_1$. Since $y_1$ prefers $x_2$, then $x_1$ leaves $y_1$. $M$ becomes $\left(x_2, y_1\right)$.

   We can pick $x_1$ again. $y_1$ is no longer on its list, so let's look at $y_2$. Since it is not paired yet, $M$ becomes $\left(x_2, y_1\right), \left(x_1, y_2\right)$.

   We can now pick $x_3$. $y_1$ is already paired and $x_2$ is before $x_3$ in $L_{y_1}$, then $x_3$ removes $y$ from $L_{x_3}$. Considering $y_2$ now, since it prefers $x_3$ than $x_1$, $x_1$ leaves $y_2$, and $M$ becomes $\left(x_2, y_1\right), \left(x_3, y_2\right)$.

   Picking $x_1$ again. The first element on its list is $y_3$ and it is not paired yet, so $M$ becomes $\left(x_2, y_1\right),\left(x_3,y_2\right),\left(x_1,y_3\right)$.

   We can continue applying this reasoning over $x_4$.

   At the end, the algorithm will give us the optimal solution.
}




\end{document}
